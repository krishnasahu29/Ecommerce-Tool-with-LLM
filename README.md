# ğŸ  ECommerce Tool with LLM

This project is an **AI-powered analytics and insights tool** that leverages **Large Language Models (LLMs) via Groq** to analyze, summarize, and generate insights from women's footwear data scraped from **Flipkart**. It helps e-commerce teams, sellers, and analysts understand product trends, pricing, sentiment, and competitive landscape.

---

## ğŸš€ Features

- ğŸ” **Web Scraping**: Collects live product data (title, price, brand, rating, reviews) from Flipkart women's shoes category.
- ğŸ§  **LLM-Powered Insights**: Uses Groq-hosted LLMs to generate insights like:
  - Popular brands
  - Price range distribution
  - Top-rated products
  - Summary of customer sentiment
- ğŸ“Š **Interactive Reports**: Outputs insights in clean markdown, JSON, or optional HTML dashboard.
- ğŸ› ï¸ **Extensible**: Easily extendable to other categories like men's shoes, clothing, electronics, etc.

---

## ğŸ› ï¸ Technology Stack

Frontend: Streamlit AI Framework: LangChain Vector Database: Chroma LLM: Groq/Llama Embeddings: Sentence Transformers Data Sources: CSV Files through Web Scraping Backend: Python, FastAPI (optional)

## ğŸ“‹ Prerequisites

Python 3.8+ Groq API key (or other LLM provider) Real estate URLs Git

## ğŸ”§ Installation

Clone the repository

bashgit clone https://github.com/krishnasahu29/Ecommerce-Tool-with-LLM

Create virtual environment

bashpython -m venv venv source venv/bin/activate # On Windows: venv\Scripts\activate

Install dependencies

bashpip install -r requirements.txt

Set up environment variables

bashcp .env.example .env

Edit .env with your API keys and configuration
Initialize vector database

bashpython scripts/setup_vectordb.py

## âš™ï¸ Configuration

Create a .env file with the following variables: envOPENAI_API_KEY=your_openai_api_key STREAMLIT_SERVER_PORT=8501 VECTOR_DB_PATH=./data/vectordb REAL_ESTATE_API_KEY=your_real_estate_api_key EMBEDDING_MODEL=all-MiniLM-L6-v2 LLM_MODEL=llama-3.3-70b-versatile

## ğŸš€ Usage

Start the Streamlit application

bashstreamlit run app.py

Access the application Open your browser and navigate to http://localhost:8501 Upload URLs

## ğŸ¤– RAG Implementation Details

Document Processing Pipeline

Document Ingestion: URLs Text Chunking: Semantic chunking with overlap for context preservation Embedding Generation: Convert text chunks to vector embeddings Vector Storage: Store embeddings in vector database with metadata

Retrieval Strategy

Semantic Search: Find most relevant documents using cosine similarity Hybrid Search: Combine semantic and keyword-based search Re-ranking: Post-retrieval re-ranking for improved relevance Context Window Management: Optimize retrieved context for LLM input

Generation Enhancement

Prompt Engineering: Specialized prompts for real estate domain Context Integration: Seamlessly blend retrieved information with user queries Source Attribution: Track and display information sources Hallucination Mitigation: Grounding responses in retrieved documents

## ğŸ“ Project Structure

ecommerce-tool/
â”‚
â”œâ”€â”€ app/                           # Streamlit UI and app logic
â”‚   â”œâ”€â”€ __pycache__/
â”‚   â”œâ”€â”€ db.sqlite
â”‚   â”œâ”€â”€ main.py                    # Streamlit chat interface (LLM-powered)
â”‚   â”œâ”€â”€ router.py                  # Route logic (if needed for modularity)
â”‚   â”œâ”€â”€ sql.py                     # DB queries for products
â”‚   â”œâ”€â”€ faq.py                     # Optional FAQ bot logic
â”‚   â”œâ”€â”€ resources/                 # Logos, images, icons
â”‚       â”œâ”€â”€ ecommerce_data_final.csv   # Finalized scraped data
â”‚       â”œâ”€â”€ faq_data.csv               # Few faq questions fed to the chatbot
â”‚       â”œâ”€â”€ architecture-diagram.png   # Architecture for the project
â”‚       â””â”€â”€ product-ss.png             # Screenshot of the Frontend
â”‚
â”œâ”€â”€ data/                          # Structured product datasets
â”‚   â”œâ”€â”€ flipkart_product_data.csv
â”‚   â”œâ”€â”€ flipkart_product_links.csv
â”‚   â”œâ”€â”€ unavailable_products.csv
â”‚   â”œâ”€â”€ flipkart_data_extraction.ipynb
â”‚   â”œâ”€â”€ duplicate_products.csv
â”‚   â””â”€â”€ csv_to_sqlite.py          # Convert CSV â†’ SQLite
â”‚
â”œâ”€â”€ requirements.txt               # Python dependencies
â””â”€â”€ README.md                      # Project overview

## ğŸ“ˆ Example Output

![image](https://github.com/user-attachments/assets/c935d369-3b53-4411-b347-3efec4bd8280)


## ğŸ¤ Contributing

Fork the repository Create a feature branch (git checkout -b feature/amazing-feature) Commit your changes (git commit -m 'Add amazing feature') Push to the branch (git push origin feature/amazing-feature) Open a Pull Request

## ğŸ™ Acknowledgments

Groq for providing powerful language models Streamlit team for the excellent web framework LangChain/LlamaIndex communities for RAG tools Real estate data providers and APIs

## ğŸ“ Support 

If you have any questions or need support, please reach out on: GitHub: @krishnasahu29 LinkedIn: www.linkedin.com/in/krishnasahu29 Email: krishna.sahu.work222@gmail.com

